{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663ae2e-88d4-41d7-a31b-28c1240f8d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models.resnet import ResNet50_Weights\n",
    "\n",
    "\n",
    "from utils.dataset import SkinDataset\n",
    "from utils.utils import train, validate, test, load_data_file\n",
    "from utils.metric import MetricsMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14ab6e-615a-48bf-8709-55251f138bc0",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7351166-ac79-42b0-8e12-79c4b173142c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally with 50% probability\n",
    "    transforms.RandomAffine(\n",
    "        degrees=15,                           # Random rotation within [-15, 15] degrees\n",
    "        translate=(0.1, 0.1),                 # Random shift by up to 10% in both x and y directions\n",
    "    ),\n",
    "    transforms.Resize((224, 224)),           # Resize to match ResNet input size\n",
    "    transforms.ToTensor(),                   # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3349c3-cdad-4881-b5c5-8dfddcebb3a5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c240c1-40f8-4248-a383-2b3eade71e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLASSES = ['nevus', 'others']\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LR = 0.0005\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5b566c-0dc0-4617-a3a6-a17667754e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load data paths and labels\n",
    "train_path, train_labels = load_data_file('datasets/train.txt')\n",
    "train_path, val_path, train_labels, val_labels = train_test_split(train_path, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "test_path, test_labels = load_data_file('datasets/val.txt')\n",
    "\n",
    "## Create datasets and dataloaders\n",
    "train_dataset = SkinDataset(train_path, train_labels, transform)\n",
    "val_dataset = SkinDataset(val_path, val_labels, transform)\n",
    "test_dataset = SkinDataset(test_path, test_labels, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93041416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Train dataset Info: ==================\n",
      " Dataset: 12156 samples\n",
      "Class distribution: {0: 6180, 1: 5976}\n",
      "\n",
      "================== Val dataset Info: ==================\n",
      " Dataset: 3039 samples\n",
      "Class distribution: {1: 1494, 0: 1545}\n",
      "\n",
      "================== Test dataset Info: ==================\n",
      " Dataset: 3796 samples\n",
      "Class distribution: {0: 1931, 1: 1865}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('================== Train dataset Info: ==================\\n', train_dataset)\n",
    "print('================== Val dataset Info: ==================\\n', val_dataset)\n",
    "print('================== Test dataset Info: ==================\\n', test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ecbcaf-c933-4a25-947d-4c60d35d62ca",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5cfc8-0e48-4986-9ad5-72a10efdda18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huytrq/miniconda3/envs/py11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/huytrq/miniconda3/envs/py11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(CLASSES))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8adbce-010e-4a81-8c43-9b958e9538d2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9c4299-8a3f-41cd-95e6-86df310cddea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Monitors\n",
    "train_monitor = MetricsMonitor(metrics=[\"loss\", \"accuracy\"])\n",
    "val_monitor = MetricsMonitor(metrics=[\"loss\", \"accuracy\"], patience=5, mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50415a4-b0b3-4f5b-a7f4-9423ebc93c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train(model, train_loader, criterion, optimizer, device, train_monitor)\n",
    "    validate(model, val_loader, criterion, device, val_monitor)\n",
    "    val_acc = val_monitor.compute_average(\"accuracy\")\n",
    "    if val_monitor.early_stopping_check(val_acc, model):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb014470",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f7c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Load the notebook\n",
    "notebook_path = \"Binary.ipynb\"\n",
    "output_script_path = \"exp.py\"\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Extract code cells only\n",
    "code_cells = [cell['source'] for cell in notebook.cells if cell.cell_type == 'code']\n",
    "code_cells = code_cells[:-1]  # Exclude the last cell\n",
    "\n",
    "# Save as a .py file\n",
    "with open(output_script_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\\n\".join(code_cells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914a67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b11a5fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting basic-image-eda\n",
      "  Downloading basic_image_eda-0.0.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Downloading basic_image_eda-0.0.3-py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: basic-image-eda\n",
      "Successfully installed basic-image-eda-0.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install basic-image-eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a948cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "found 18991 images.\n",
      "Using 16 threads. (max:16)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18991/18991 [01:29<00:00, 213.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*--------------------------------------------------------------------------------------*\n",
      "number of images                         |  18991\n",
      "\n",
      "dtype                                    |  uint8\n",
      "channels                                 |  [3]\n",
      "extensions                               |  ['jpg']\n",
      "\n",
      "min height                               |  450\n",
      "max height                               |  1024\n",
      "mean height                              |  761.2003580643462\n",
      "median height                            |  768\n",
      "\n",
      "min width                                |  576\n",
      "max width                                |  1024\n",
      "mean width                               |  855.1536517297667\n",
      "median width                             |  1024\n",
      "\n",
      "mean height/width ratio                  |  0.8901328510082651\n",
      "median height/width ratio                |  0.75\n",
      "recommended input size(by mean)          |  [760 856] (h x w, multiples of 8)\n",
      "recommended input size(by mean)          |  [768 848] (h x w, multiples of 16)\n",
      "recommended input size(by mean)          |  [768 864] (h x w, multiples of 32)\n",
      "\n",
      "channel mean(0~1)                        |  [0.6665315  0.52944165 0.5243559 ]\n",
      "channel std(0~1)                         |  [0.22581616 0.20484802 0.21641603]\n",
      "*--------------------------------------------------------------------------------------*\n",
      "eda ended in 00 hours 01 minutes 29 seconds\n",
      "No images found.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from basic_image_eda import BasicImageEDA\n",
    "\n",
    "if __name__ == \"__main__\":  # for multiprocessing\n",
    "    data_dir = \"./data\"\n",
    "    BasicImageEDA.explore('/root/huy/datasets/Binary')\n",
    "        \n",
    "    # or\n",
    "    \n",
    "    extensions = ['png', 'jpg', 'jpeg']\n",
    "    threads = 0\n",
    "    dimension_plot = True\n",
    "    channel_hist = True\n",
    "    nonzero = False\n",
    "    hw_division_factor = 1.0\n",
    "    \n",
    "    BasicImageEDA.explore(data_dir, extensions, threads, dimension_plot, channel_hist, nonzero, hw_division_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35aed4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ec2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
